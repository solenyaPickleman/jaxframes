An Architectural Blueprint for JaxFrame: A Scalable DataFrame Library on TPUs with JAX
Introduction and Vision
The Case for an Accelerator-Native DataFrame
The modern data science and machine learning landscape is characterized by a fundamental architectural divide. On one side, highly optimized DataFrame libraries such as Polars and NVIDIA cuDF provide exceptional performance for data manipulation on CPUs and GPUs, respectively. On the other, frameworks like JAX offer a powerful paradigm for building and training large-scale models on accelerators, particularly Google's Tensor Processing Units (TPUs). A critical gap exists between these two worlds. Currently, workflows that involve extensive data preprocessing before model training on TPUs require an inefficient handoff: data is manipulated using a traditional library and then transferred to the accelerator, breaking the potential for end-to-end optimization and creating performance bottlenecks.
JaxFrame is envisioned to bridge this gap. It is an architectural proposal for a new DataFrame library designed not merely to be compatible with JAX, but to be fundamentally of JAX. This approach enables seamless, high-performance workflows where both complex data transformations and model training can be expressed and executed within a single, unified, accelerator-native framework.
Core Vision for JaxFrame
The central vision for JaxFrame is to create a DataFrame library that fully embraces and extends the JAX paradigm. This means the library will be architected around four core principles:
 * Functionally Pure: All operations will be implemented as pure functions with no side effects. This aligns with JAX's core design, which eschews mutable state and global variables in favor of explicit state management, a prerequisite for its powerful transformations.
 * JIT-Compilable: Entire sequences of DataFrame operations—from filtering and aggregation to complex joins—will be composed into a single function that can be just-in-time (JIT) compiled by JAX into a highly optimized XLA (Accelerated Linear Algebra) executable. This enables whole-program optimization that is impossible in traditional eager-execution libraries.
 * Inherently Scalable: The library will be designed from the ground up for distributed execution across multi-TPU pods. It will leverage JAX's explicit parallelism models, such as shard_map, to give developers fine-grained control over data sharding and communication patterns, enabling performance at massive scale.
 * Composable: DataFrame transformations will be treated as composable functions, just like any other JAX operation. This will allow them to be seamlessly mixed and composed with JAX's other transformations, such as automatic differentiation (jax.grad) and vectorization (jax.vmap), opening up new possibilities for differentiable data preprocessing pipelines.
Key Differentiators
JaxFrame's architecture represents a fundamental departure from existing DataFrame libraries. Traditional libraries like pandas or cuDF operate by having an interpreted Python layer dispatch pre-compiled, high-performance kernels for each operation (e.g., a filter kernel, a groupby kernel). In this model, the Python code serves as "glue," and optimization is limited to the scope of a single kernel.
In contrast, JaxFrame's Python API layer is not merely glue; it is an integral part of the computation graph to be compiled. When a user chains together a series of JaxFrame operations, they are defining a single, large computational graph. JAX's JIT compiler, powered by XLA, can then perform whole-program optimizations across the entire sequence of operations, fusing multiple element-wise operations into a single kernel, reordering computations to improve memory access patterns, and eliminating redundant work. This is a level of optimization that is architecturally inaccessible to libraries built on an eager, kernel-dispatch model. Similarly, while a library like Dask parallelizes existing pandas operations across a cluster, JaxFrame will use JAX's native primitives to define fine-grained parallel execution patterns directly on the accelerator hardware.
Foundational Tenets: Building on the JAX Paradigm
The core constraints of the JAX framework—immutability, its specific object model, and its compilation-centric nature—are not limitations to be worked around. Instead, they are powerful design principles that, when embraced, naturally lead to a more robust, scalable, and performant architecture for a parallel DataFrame library.
Tenet 1: Immutability and Functional Purity
A core principle of JAX is that its arrays are immutable. An operation like x = 10, which modifies an array in-place in NumPy, is disallowed. Instead, one must use a functional equivalent like y = x.at.set(10), which returns a new array y with the updated value, leaving the original array x untouched. This principle of functional purity, where functions have no side effects, is essential for the correctness and performance of JAX's transformations.
This tenet has a profound architectural implication: it makes traditional, imperative DataFrame APIs (e.g., df['new_col'] = df['a'] + df['b']) fundamentally incompatible. It forces the adoption of a declarative, expression-based API, where operations describe a new DataFrame to be created rather than modifying an existing one. This constraint is the primary enabler of whole-program optimization.
The chain of dependencies is direct and causal. Traditional DataFrame libraries that allow mutable, in-place operations introduce hidden state changes and side effects. Compilers like XLA achieve their remarkable performance by aggressively reordering, fusing, and eliminating operations in a computational graph. These optimizations are only safe if the compiler can mathematically guarantee that changing the order of operations will not alter the final result—a property known as referential transparency. Immutability and functional purity provide this exact guarantee. Because no function can have side effects, the execution order of independent operations is irrelevant to the outcome, granting the compiler complete freedom to discover and implement the most efficient execution path. Therefore, JAX's immutability is not merely a feature; it is the foundational prerequisite that unlocks XLA's powerful optimization capabilities for complex, multi-step DataFrame queries.
Tenet 2: The DataFrame as a PyTree
JAX transformations like jit and grad are designed to operate on a specific set of types: JAX arrays and standard Python containers such as lists, tuples, and dictionaries. To make a custom Python class or object compatible with these transformations, it must be registered as a "PyTree". This registration process involves telling JAX how to deconstruct the object into its constituent parts (flattening) and how to reconstruct it from those parts (unflattening).
For JaxFrame, the JaxFrame class itself will be registered as a PyTree node. This registration is the critical bridge between the user-facing object abstraction and JAX's underlying computational engine.
 * tree_flatten: This method will deconstruct a JaxFrame instance into two components. The "children," which represent the dynamic data that JAX will trace and transform, will be a tuple containing the jax.Array objects for each column. The "auxiliary data," which represents the static, non-array metadata, will be a hashable structure (like a tuple or frozendict) containing information such as column names, their order, and index properties.
 * tree_unflatten: This method will perform the reverse operation, reconstructing a JaxFrame object from the static auxiliary data and a tuple of (potentially transformed) column arrays.
This PyTree mechanism elegantly solves the problem of integrating a complex, high-level data structure into JAX's ecosystem. A user interacts with a familiar JaxFrame object, calling methods like df.select(...). When this object is passed into a jit-compiled function, JAX automatically invokes tree_flatten, extracts the column arrays (the "leaves" of the tree), and traces the operations on these arrays to build its internal jaxpr representation. After the transformed computation is complete, JAX uses tree_unflatten to reassemble the resulting arrays into a new JaxFrame object. This process makes the JaxFrame object effectively "transparent" to JAX's transformations, allowing the user to work with a convenient abstraction while JAX operates on the raw arrays where performance is paramount.
Tenet 3: Lazy Execution by Default
Inspired by the highly efficient architecture of modern DataFrame libraries like Polars, JaxFrame will adopt a lazy execution model by default. When a user calls a method like .filter() or .groupby(), the operation is not executed immediately. Instead, these calls build up a logical query plan, which is a data structure that represents the sequence of desired transformations. The actual computation is only triggered when the user explicitly requests a result, for instance by calling a .collect() method.
This approach is the natural consequence of the first two tenets. The chain of expression-based API calls on an immutable data structure defines a pure function. The lazy query plan is the representation of this composed function. The .collect() method then serves as the trigger that passes this function to JAX's transformations, like jax.jit and shard_map, for compilation and execution. This allows a full query to be optimized as a single unit before any computation occurs.
| Feature | pandas | cuDF | JaxFrame (Proposed) |
|---|---|---|---|
| Core Data Model | Mutable numpy.ndarray | Mutable cupy.ndarray  | Immutable jax.Array  |
| Execution Model | Eager (op-by-op) | Eager (kernel-by-kernel)  | Lazy (JIT-compiled query plan)  |
| Parallelism | Single-threaded | Implicitly parallel within kernels | Explicitly parallel via SPMD (shard_map)  |
| Optimization | Manual/Kernel-level | Manual/Kernel-level | Whole-program (cross-operation fusion via XLA)  |
| API Style | Imperative | Imperative | Declarative/Functional (Expression-based) |
The JaxFrame Architectural Blueprint
The architecture of JaxFrame's lazy execution engine adapts proven concepts from high-performance query engines like Polars and DuckDB and recasts them within the JAX programming model. The engine consists of four main stages: Expression API, Logical Query Plan, Query Optimizer, and Physical Plan Generation.
The Expression API
The user-facing API will be heavily inspired by the highly expressive and intuitive API of Polars. Users will construct queries by composing expressions on columns. For example, creating a new column based on the values of two existing columns would be written as:
df = df.with_columns(
    new_col = jf.col('a') * 2 + jf.col('b').log()
)

Each component of this expression—jf.col('a'), the multiplication operator, the .log() method—is not a direct computation. Instead, each is a Python object that represents a node in an expression tree. These objects are lightweight and contain no actual data; they are simply a recipe for a computation that will be incorporated into the larger query plan.
The Logical Query Plan
As the user chains DataFrame methods like .filter(), .select(), .join(), and .groupby(), JaxFrame constructs a logical query plan. This plan is a tree-like data structure, much like the one used internally by Polars, where nodes represent high-level relational operations (Selection, Projection, Join, Aggregate) and the leaves represent data sources (e.g., ScanParquet). The JaxFrame object itself is merely a lightweight handle that points to the current final node of this evolving plan. The plan describes what computation the user wants to perform, but not how to execute it efficiently.
The Query Optimizer
Before execution, the logical query plan is passed to a query optimizer. The optimizer's role is to traverse and rewrite the logical plan into a new, logically equivalent plan that will be significantly more performant. This is a critical step that enables high performance without requiring the user to manually write optimized code. Key optimization passes will include:
 * Predicate Pushdown: This fundamental optimization moves filter operations as close to the data source as possible in the plan tree. In a distributed TPU environment, this is paramount. It means that data can be filtered on each TPU host before it is shuffled across the high-speed interconnect for operations like joins or aggregations. This can drastically reduce the volume of data communicated between devices, which is often a primary performance bottleneck.
 * Projection Pushdown: The optimizer analyzes the entire query to determine the minimal set of columns required to satisfy the final output. This information is then "pushed down" to the data source node, ensuring that only the necessary columns are ever read from storage (e.g., Google Cloud Storage) and loaded into the limited High-Bandwidth Memory (HBM) of the TPUs.
 * Expression Simplification and Constant Folding: Similar to the expression rewriter in DuckDB, this pass performs algebraic simplifications (e.g., rewriting x * 1 to x) and pre-computes expressions that depend only on constant values (e.g., 2 + 2 becomes 4). This reduces the amount of work that needs to be done by the accelerators at runtime.
The role of the JaxFrame optimizer extends beyond that of a traditional database optimizer. A standard optimizer translates a logical plan into a physical plan, which is a sequence of specific algorithms (e.g., "use hash join," "use merge sort"). In JaxFrame, the output of the optimization process is not just a sequence of steps; it is the source code of a complete, JIT-compilable JAX function.
This means the optimizer must function as a JAX meta-programmer. It must be aware of JAX-specific performance idioms. For example, it might reorder a series of element-wise operations to be adjacent in the generated code, knowing that the XLA compiler is exceptionally effective at fusing them into a single, efficient kernel. It could also employ a cost model to decide which jax.lax collective communication primitive is most appropriate for a given data redistribution pattern. For a join operation, the optimizer would be responsible for generating the Python code that calls the necessary all_to_all repartitioning function and the local join function. In essence, the optimizer is not just planning the query; it is actively writing expert-level JAX code tailored to that specific query.
Physical Plan Generation and Execution
The final stage is the translation of the optimized logical plan into a physical plan and its execution. The "physical plan" is the single, pure Python function generated by the optimizer, which uses JAX NumPy (jnp) and jax.lax primitives to express the entire query.
The user-facing .collect() method orchestrates the execution. It takes the generated function, wraps it with the appropriate JAX transformations (jax.jit for compilation and jax.shard_map for distribution), and executes it across the TPU pod. The inputs to this compiled function are the sharded jax.Array columns read from the data source, and the output is the final JaxFrame result, also represented as a set of sharded jax.Arrays.
Core Algorithms for Distributed Execution on TPUs
The query planner and optimizer rely on a library of highly efficient, distributed algorithms for core DataFrame operations. These algorithms must be implemented from scratch using JAX's primitives to run natively on TPUs.
Distributed Sorting: The Foundational Primitive
A robust, high-performance distributed sort is the most critical foundational primitive for the entire library. The proposed algorithm is a distributed, multi-pass, least-significant-digit (LSD) first radix sort, an approach well-suited for the massive parallelism of hardware like GPUs and TPUs.
The JAX implementation would be structured as a function wrapped in shard_map, where each device initially holds a shard of the column to be sorted. The sort proceeds in passes, one for each chunk of bits in the keys (e.g., 8 bits at a time). Within each pass, the following steps occur:
 * Local Count: Each TPU core computes a local histogram of the current bit-chunk from its local data shard. This is an embarrassingly parallel operation.
 * Global Count & Offsets: A jax.lax.all_gather collective is used to collect the local histograms from all devices onto a designated device (or all devices). A global prefix sum (scan) is then computed on these aggregated counts to determine the global starting offset for each key value for each device shard.
 * Reorder/Shuffle: A jax.lax.all_to_all collective performs the main data exchange. Each device uses the global offsets to calculate the destination device for each of its local keys and sends them accordingly.
This process repeats for subsequent bit-chunks until the entire key space has been processed and the column is globally sorted across all devices.
By investing heavily in the optimization of this single primitive, the implementation of many other complex operations becomes significantly simpler. Numerous challenging DataFrame operations, including groupby-aggregation and sort-merge-join, are difficult to parallelize efficiently due to random data access patterns and synchronization overhead. However, if the input data is first sorted by the relevant key (the group key or join key), these operations transform into embarrassingly parallel local computations or simple, sequential memory scans. This sort-centric design, where complex operations are decomposed into a sort followed by a simpler, communication-light operation, makes the overall library more modular, performant, and robust.
Group-By and Aggregation via Segmented Reduction
The groupby-aggregation operation is a direct application of this sort-centric design. An operation like df.groupby('key').agg(sum('value')) is implemented as a two-phase process:
 * Phase 1: Sort. The distributed radix sort primitive is used to sort the required columns (at a minimum, the key and value columns) by the group key. After this step, all rows belonging to the same group are guaranteed to be on the same device and stored contiguously in memory.
 * Phase 2: Segmented Reduction. A second shard_map operation is executed. Since each device now holds sorted, contiguous segments (one for each group key present on that device), a parallel reduction primitive, such as jax.lax.associative_scan, can be run locally on these segments to compute the aggregate (e.g., sum, mean, max) for each group. This phase requires no further cross-device communication, making it highly efficient.
Distributed Joins: Prioritizing Sort-Merge
For join operations, the architecture will prioritize a distributed sort-merge join strategy. This choice goes against classical wisdom, which often favored hash joins for in-memory databases, but is based on a careful analysis of modern hardware trends and the specific characteristics of TPUs.
TPUs are massively parallel vector processors whose performance is contingent on a steady stream of data from HBM to their computational units. They excel with predictable, sequential memory access patterns that allow hardware prefetchers to work effectively. Hash joins consist of a "build" phase and a "probe" phase. The probe phase involves random memory accesses into the hash table to find matching keys. This access pattern is fundamentally at odds with the strengths of a vector processor, as it can lead to cache misses, TLB misses, and memory stalls that leave the computational units idle.
Sort-merge joins, in contrast, have an execution profile that is architecturally aligned with TPU hardware. While they have a higher upfront computational cost for sorting, this sorting phase (using the radix sort described above) is itself highly parallelizable. The final merge phase is a simple linear scan over two sorted arrays—a perfectly sequential memory access pattern. Research indicates that as the width of SIMD/vector units increases and the memory bandwidth per core becomes a more significant bottleneck, the performance balance shifts in favor of sort-merge joins. TPUs represent an extreme point on this hardware trend curve, making sort-merge the more forward-looking and architecturally appropriate choice.
The JAX implementation of a distributed sort-merge join would consist of two main phases:
 * Phase 1: Repartitioning (Shuffle). This is the most communication-heavy step. A hash function is applied to the join key of both DataFrames to determine a target device for each row. A jax.lax.all_to_all collective then shuffles the rows across the TPU interconnect, ensuring that all rows from both tables with the same range of join keys land on the same device. This is the distributed equivalent of the partitioning phase in parallel database literature.
 * Phase 2: Local Sort-Merge. A shard_map operation is applied across the devices. On each device, the now co-located data shards are sorted locally by the join key. A standard, highly efficient sort-merge join algorithm—which involves iterating through the two sorted arrays with pointers—is then performed entirely locally on each device, requiring no further cross-device communication.
| Criterion | Distributed Sort-Merge Join | Distributed Hash Join |
|---|---|---|
| Memory Access Pattern | Primarily sequential (sorting, merging) | Highly random (probe phase) |
| TPU/Vector Unit Suitability | High (friendly to prefetching and vectorization) | Lower (potential for stalls due to random access) |
| Communication Pattern | Two major phases: all_to_all shuffle, then local work | One major phase: all_to_all shuffle, then local work  |
| Memory Bandwidth Demand | Lower, more predictable  | Higher, bursty |
| Sorted Output | Yes (can be exploited by downstream operations) | No |
| Conclusion | Architecturally aligned with TPU hardware strengths. | Misaligned with TPU memory access strengths. |
The Distributed Execution Model
Mapping the logical operations and algorithms onto the physical TPU hardware is managed by JAX's powerful distribution APIs.
Data Sharding and Layout with DeviceMesh and PartitionSpec
The foundation of distributed execution in JAX is the explicit definition of data layout. A logical topology of the available TPU devices is created using jax.sharding.Mesh, which arranges devices into a named, multi-dimensional grid (e.g., mesh = Mesh(devices, ('data', 'model'))). The layout of a jax.Array across this mesh is then described using jax.sharding.PartitionSpec (often aliased as P).
Within JaxFrame, each column will be represented by a jax.Array associated with a NamedSharding object that combines a Mesh and a PartitionSpec. The default and most common sharding strategy will be row-wise partitioning. For a mesh with a 'data' axis used for data parallelism, this would be specified as NamedSharding(mesh, P('data', None)). This instruction tells JAX to split the array's first axis (the rows) across the 'data' axis of the device mesh and to replicate the array across all other mesh axes.
SPMD Programming with shard_map
All core distributed algorithms in JaxFrame will be implemented as functions designed to be wrapped by jax.shard_map. This function transformation provides an explicit Single-Program, Multiple-Data (SPMD) programming model, which is a more expressive and controllable evolution of the older pmap API. shard_map gives the developer precise control over how input arrays are split across the mesh (in_specs), how the body of the function is executed on each data shard, and how the results from each device are reassembled into a final output (out_specs). This explicit approach is preferable to relying on the compiler to automatically infer parallelism, as automatic parallelization can sometimes produce suboptimal communication patterns or fail on complex operations.
A Catalogue of Collective Communication Patterns
The implementation of distributed DataFrame operations maps directly to a set of jax.lax collective communication primitives. These primitives are the building blocks for moving data between TPUs over the high-speed interconnect.
 * Global Aggregation (df.sum()): A local reduction (e.g., jnp.sum) is performed on each device's data shard. The resulting scalars are then summed across all devices using jax.lax.psum to produce the final global sum.
 * Broadcast Join (df.join(small_df)): When joining a large DataFrame with a small one, the most efficient strategy is often to make the small DataFrame available on every device. This is achieved by using jax.lax.all_gather to collect all shards of the small DataFrame and replicate the full table on each device, after which a purely local join can be performed.
 * Shuffle (df.join(large_df), df.sort()): Operations that require a global re-organization of data based on key values, such as the repartitioning phase of a join or a radix sort, are implemented using jax.lax.all_to_all. This collective allows each device to send a distinct piece of its local data to every other device in the mesh.
 * Window Functions (lag, lead): For operations that need access to data from neighboring rows, such as calculating a lagged value, a full shuffle is often unnecessary. jax.lax.ppermute can be used to efficiently shift data between adjacent devices in a mesh (e.g., device i sends its data to device i+1), enabling these computations with minimal communication.
| User API Call | Logical Plan Operation | Core JaxFrame Algorithm | Key JAX Primitives |
|---|---|---|---|
| df.filter(df['a'] > 0) | Selection | Predicate Pushdown | jnp.where (local) |
| df['a'].sum() | GlobalAggregate | Segmented Reduction (single segment) | jnp.sum (local), lax.psum |
| df.groupby('b').agg(sum('a')) | Aggregate | Sort + Segmented Reduction | distributed_radix_sort, lax.associative_scan |
| df.join(other, on='c') | Join | Distributed Sort-Merge Join | lax.all_to_all, local_sort, local_merge |
Ecosystem Integration and Practical Considerations
Data Ingestion: Parallel Parquet Reader
A significant real-world bottleneck is efficiently loading data from persistent storage (like Google Cloud Storage) onto a distributed set of TPU devices. To address this, JaxFrame will require a parallel data reader. The design can be inspired by Dask's read_parquet implementation. A coordinator process on the main host would read the Parquet file metadata to identify the byte offsets of the different row groups. This information would then be broadcast to all TPU hosts. Each host would then be responsible for independently reading its assigned subset of row groups directly from the file, decoding them, and transferring the resulting arrays to its attached TPU devices. This parallelizes the I/O and decoding workload, which is essential for achieving high throughput.
Interoperability
To be a useful component of the broader ecosystem, JaxFrame must provide seamless interoperability with other JAX data structures. It will expose two key methods:
 * jf.from_jax(array_dict, sharding): A constructor to create a distributed JaxFrame from a dictionary of jax.Arrays, allowing users to bring data that already exists on-device into the JaxFrame paradigm.
 * df.to_jax(): A method to extract the underlying column data as a PyTree of sharded jax.Arrays.
The design of these methods will draw inspiration from existing converters, such as Polars' to_jax method and the JAX format supported by HuggingFace Datasets, to ensure a familiar and efficient interface.
Debugging and Profiling Distributed Queries
Debugging JIT-compiled, distributed programs presents unique challenges, as standard Python tools like pdb cannot inspect intermediate values that exist only on the accelerator during a compiled execution. The strategy for JaxFrame must therefore be two-fold:
 * Debugging: For development and logic verification, JaxFrame will support an "eager" or "disabled JIT" mode, allowing users to fall back to standard Python debugging tools. For issues that only manifest under JIT compilation, developers must use JAX-native debugging primitives. jax.debug.print and jax.debug.breakpoint are designed to be staged out by the compiler and work correctly inside jit and shard_map, allowing inspection of on-device values at runtime.
 * Profiling: Performance analysis will rely on JAX's built-in profiler, which integrates with the TensorBoard plugin (XProf). This tool can capture detailed performance traces from the TPUs, providing a timeline view of execution on each device. This visualization is essential for identifying performance bottlenecks, such as excessive time spent in collective communication operations or load imbalances between devices. For memory-specific issues, tools like jax-smi can be used to monitor the HBM usage on each TPU device.
Conclusion and Future Directions
Summary of the JaxFrame Blueprint
This report has outlined an architectural blueprint for JaxFrame, a scalable DataFrame library designed to be a first-class citizen in the JAX ecosystem for high-performance computing on TPUs. The core of the architecture is a set of design principles derived directly from the JAX paradigm: a lazy, expression-based API built upon an immutable data model; a JaxFrame object registered as a JAX PyTree to be transparent to JAX transformations; a JAX-aware query optimizer that acts as a meta-programmer to generate efficient, JIT-compilable code; and a library of core distributed algorithms, founded on a highly optimized radix sort primitive and executed via the explicit shard_map SPMD model. This architecture is purpose-built to leverage the strengths of JAX and the underlying TPU hardware, enabling end-to-end, optimized data science workflows on accelerators.
Phased Implementation Roadmap
A practical implementation of JaxFrame could proceed in phases to manage complexity:
 * Phase 1 (Core Engine): Implement the JaxFrame PyTree registration, the expression API objects, and the basic lazy query plan data structure. Implement simple element-wise operations and a basic .collect() method that can JIT-compile and execute them.
 * Phase 2 (Sorting and Aggregation): Develop and optimize the distributed radix sort primitive. With the sort in place, implement the sort-based groupby-aggregation algorithm.
 * Phase 3 (Joins and I/O): Implement the distributed sort-merge join algorithm, including the all_to_all repartitioning step. Develop the parallel Parquet reader for efficient data ingestion.
 * Phase 4 (Optimization): Build out the query optimizer, starting with predicate and projection pushdown, and gradually adding more sophisticated rewrite rules.
Future Research Directions
The JaxFrame architecture opens several avenues for future research and development:
 * Advanced Cost-Based Optimization: The query optimizer could be enhanced with a sophisticated cost model that is aware of the TPU pod topology (e.g., the cost of communication between chips on the same host vs. different hosts) and the precise performance characteristics of different jax.lax collective primitives for various data sizes.
 * Hardware-Specific Kernel Fusion: For common sequences of operations that are not automatically fused by XLA, JAX's lower-level Pallas extension could be used to write custom, hand-optimized kernels that fuse multiple logical operations (e.g., a filter, a projection, and an aggregation) into a single TPU kernel for maximum performance.
 * Streaming and Out-of-Core Processing: A significant challenge is handling datasets that exceed the total available HBM of the entire TPU pod. Future work could investigate extending the lazy execution model to support streaming, where data is processed in chunks that are streamed from storage through the compiled computation graph, allowing JaxFrame to operate on datasets of arbitrary size.
